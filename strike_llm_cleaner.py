#!/usr/bin/env python3
"""
Strike LLM Cleaner

This script processes JSON files generated by raw_strike_description_collector.py
and uses OpenAI API to extract structured strike data from the newspaper content.

Usage: python strike_llm_cleaner.py <input_folder> <output_folder>
"""

import json
import os
import sys
import re
import argparse
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime
import openai
from openai import OpenAI

# Configuration - Modify these as needed
OPENAI_DATE_MODEL = "gpt-4o-mini"  # Model for date extraction (simpler task)
OPENAI_STRIKES_MODEL = "gpt-4.1-mini"  # Model for strike analysis (complex task)
MAX_RETRIES = 3
RETRY_DELAY = 1  # seconds

def setup_openai_client() -> OpenAI:
    """Initialize OpenAI client with API key from environment variable."""
    api_key = os.environ.get('OPENAI_API_KEY')
    if not api_key:
        print("âŒ Error: OPENAI_API_KEY environment variable not set")
        print("   Please set it with: set OPENAI_API_KEY=your_api_key_here")
        sys.exit(1)
    
    return OpenAI(api_key=api_key)


def extract_year_from_filename(filename: str) -> Optional[str]:
    """Extract year from filename like 'toke_munka_Nepszava_1903_05__pages51-96_images_page_36.json'."""
    # Pattern: toke_munka_Nepszava_YYYY_
    match = re.search(r'toke_munka_Nepszava_(\d{4})_', filename)
    if match:
        return match.group(1)
    
    print(f"    âš ï¸  Could not extract year from filename: {filename}")
    return None


def query_openai_with_retry(client: OpenAI, messages: List[Dict], model: str, max_retries: int = MAX_RETRIES) -> Optional[str]:
    """Query OpenAI API with retry logic."""
    import time
    
    print(f"    ğŸ”„ Making OpenAI API call with model: {model}")
    print(f"    ğŸ“ Number of messages: {len(messages)}")
    for i, msg in enumerate(messages):
        print(f"    ğŸ“ Message {i+1} ({msg['role']}): {msg['content'][:100]}...")
    
    for attempt in range(max_retries):
        try:
            response = client.chat.completions.create(
                model=model,
                messages=messages,
                max_completion_tokens=8000
            )
            result = response.choices[0].message.content
            if result is None:
                print(f"    âš ï¸  OpenAI returned None as content")
                return None
            
            result = result.strip()
            print(f"    âœ… OpenAI API call successful, response length: {len(result)} characters")
            
            if len(result) == 0:
                print(f"    âš ï¸  OpenAI returned empty response!")
                print(f"    ğŸ” Full response object: {response}")
                print(f"    ğŸ” Message content was: '{response.choices[0].message.content}'")
                print(f"    ğŸ” Finish reason: {response.choices[0].finish_reason}")
                return None
            
            return result
        
        except Exception as e:
            print(f"    âš ï¸  OpenAI API error (attempt {attempt + 1}/{max_retries}): {e}")
            if attempt < max_retries - 1:
                time.sleep(RETRY_DELAY * (attempt + 1))  # Exponential backoff
            else:
                print(f"    âŒ Failed to get response after {max_retries} attempts")
                return None
    
    return None


def extract_date_from_header(client: OpenAI, newspaper_header: str, year: str, model: str) -> str:
    """Extract date information from newspaper header using OpenAI."""
    if newspaper_header == "Unknown Issue":
        return year
    
    print(f"    ğŸ—“ï¸  Extracting date from header: {newspaper_header[:50]}...")
    
    messages = [
        {
            "role": "system",
            "content": "You are extracting date information from Hungarian newspaper headers. Return only two numbers: month and day, separated by a space. If you cannot determine the exact date, return 'UNKNOWN'."
        },
        {
            "role": "user",
            "content": f"Extract the month (1-12) and day (1-31) from this Hungarian newspaper header: '{newspaper_header}'. Return only two numbers separated by a space, nothing else."
        }
    ]
    
    response = query_openai_with_retry(client, messages, model)
    if not response:
        return year
    
    # Print the exact OpenAI response for debugging
    print(f"    ğŸ” OpenAI date response: '{response}'")
    
    # Parse the response
    try:
        if response.upper() == 'UNKNOWN':
            return year
        
        parts = response.strip().split()
        if len(parts) == 2 and parts[0].isdigit() and parts[1].isdigit():
            month = int(parts[0])
            day = int(parts[1])
            
            # Validate date ranges
            if 1 <= month <= 12 and 1 <= day <= 31:
                # Create ISO 8601 date
                iso_date = f"{year}-{month:02d}-{day:02d}"
                print(f"    âœ… Extracted date: {iso_date}")
                return iso_date
    
    except (ValueError, IndexError):
        pass
    
    print(f"    âš ï¸  Could not parse date from response: {response}")
    return year


def extract_strikes_from_content(client: OpenAI, column_content: str, model: str) -> List[Dict]:
    """Extract structured strike data from column content using OpenAI."""
    print(f"    ğŸ“Š Analyzing content for strikes ({len(column_content)} characters)...")
    print(f"    ğŸ“ Content preview: {column_content[:200]}...")
    
    strike_prompt = """I am sending you a text from the "NÃ‰PSZAVA" labor journal from the early 20th century. Please read and digest it. Please check if there were strikes. If there were strikes, return a list of them as structured JSON elements, with the following informations/elements.
1) "event_date" - exact date in ISO 8601 format
2) "industry_txt" - which industry participated (in plain words)
3) "industry_SIC" - industry SIC code, if possible
4) "participants_txt" - who were the participants (in plain words)
5) "participants_ISCO" - participant ISCO code, if possible
6) "firm_name" - which is the firm where the strike happens, or whose estate (if it is an agricultural strike)
7) "location_txt" - which is the location of the strike â€“ as described there
8) "location_official" - current official name of the closest settlement
9) "location_geonames_id" - GeoNames ID of the closest settlement 
10) "strike_status" - is the strike planned/ongoing/resolved
11) "description_en" - at most 30 word description of the event in English

Don't write any scripts, just read it. Don't write any accompanying texts like 'here is the data you asked for' just send the plain data."""
    
    messages = [
        {
            "role": "system",
            "content": "You are analyzing Hungarian labor newspaper content from the early 20th century to extract strike information. You MUST return a valid JSON array. If you find strikes, return them as structured JSON elements. If you find no strikes, return an empty array []. Do not return any other text, explanations, or markdown formatting - only the JSON array."
        },
        {
            "role": "user",
            "content": f"{strike_prompt}\n\nText to analyze:\n{column_content}\n\nIMPORTANT: You must respond with a JSON array. If no strikes are found, respond with: []"
        }
    ]
    
    response = query_openai_with_retry(client, messages, model)
    if not response:
        return []
    
    # Print the exact OpenAI response for debugging
    print(f"    ğŸ” OpenAI strikes response (first 500 chars): '{response[:500]}...'")
    print(f"    ğŸ” Full OpenAI strikes response:")
    print(f"    =====================================")
    print(response)
    print(f"    =====================================")
    
    # Try to parse JSON response
    try:
        # Clean the response - remove any markdown formatting
        clean_response = response.strip()
        if clean_response.startswith('```json'):
            clean_response = clean_response[7:]
        if clean_response.endswith('```'):
            clean_response = clean_response[:-3]
        clean_response = clean_response.strip()
        
        print(f"    ğŸ” Cleaned response for JSON parsing: '{clean_response[:200]}...'")
        
        # Parse JSON
        strikes_data = json.loads(clean_response)
        
        # Ensure it's a list
        if isinstance(strikes_data, dict):
            strikes_data = [strikes_data]
        elif not isinstance(strikes_data, list):
            print(f"    âš ï¸  Unexpected response format: {type(strikes_data)}")
            return []
        
        print(f"    âœ… Found {len(strikes_data)} strike(s)")
        return strikes_data
    
    except json.JSONDecodeError as e:
        print(f"    âŒ Failed to parse JSON response: {e}")
        print(f"    ğŸ“ Response was: {response[:200]}...")
        return []


def generate_output_filename(input_filename: str) -> str:
    """Generate output filename based on input filename."""
    # Replace .json with _strikes.json
    if input_filename.lower().endswith('.json'):
        base_name = input_filename[:-5]  # Remove .json
    else:
        base_name = input_filename
    
    return f"{base_name}_strikes.json"


def process_file(client: OpenAI, input_path: str, output_path: str, input_folder: str, date_model: str, strikes_model: str) -> bool:
    """Process a single JSON file and extract strike data."""
    filename = os.path.basename(input_path)
    print(f"    ğŸ“„ Processing: {filename}")
    
    try:
        # Load input data
        with open(input_path, 'r', encoding='utf-8') as f:
            input_data = json.load(f)
        
        # Extract year from filename
        year = extract_year_from_filename(filename)
        if not year:
            print(f"    âŒ Skipping file - could not extract year")
            return False
        
        # Extract required fields
        newspaper_header = input_data.get("newspaper_header", "Unknown Issue")
        column_content = input_data.get("column_content", "")
        
        if not column_content.strip():
            print(f"    âŒ Skipping file - no column content")
            return False
        
        # Extract publication date
        publication_date = extract_date_from_header(client, newspaper_header, year, date_model)
        
        # Extract strikes from content
        strikes = extract_strikes_from_content(client, column_content, strikes_model)
        
        # Prepare output data
        output_data = {
            "publication_date": publication_date,
            "newspaper_header": newspaper_header,
            "column_content": column_content,
            "source_file": os.path.relpath(input_path, input_folder),
            "strikes": strikes,
            "processing_info": {
                "extracted_year": year,
                "total_strikes_found": len(strikes),
                "processed_at": datetime.now().isoformat(),
                "openai_date_model_used": date_model,
                "openai_strikes_model_used": strikes_model
            }
        }
        
        # Save output
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
        
        print(f"    âœ… Saved {len(strikes)} strike(s) to: {os.path.basename(output_path)}")
        return True
    
    except Exception as e:
        print(f"    âŒ Error processing {filename}: {e}")
        return False


def parse_arguments():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Process JSON files from raw_strike_description_collector.py and extract structured strike data using OpenAI API.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python strike_llm_cleaner.py input_folder output_folder
  python strike_llm_cleaner.py input_folder output_folder --force
  python strike_llm_cleaner.py input_folder output_folder --strikemodel gpt-4o --datemodel gpt-4o-mini
  python strike_llm_cleaner.py input_folder output_folder --strikemodel gpt-5-nano --datemodel gpt-4o-mini --force

Requirements:
  - OPENAI_API_KEY environment variable must be set
  - Input folder should contain JSON files from the collector script
        """
    )
    
    # Positive arguments
    parser.add_argument('input_folder', 
                       help='Folder containing JSON files from the collector script')
    parser.add_argument('output_folder', 
                       help='Folder where processed JSON files will be saved')
    
    # Optional arguments
    parser.add_argument('--force', 
                       action='store_true',
                       help='Force reprocessing of files that already exist')
    parser.add_argument('--strikemodel', 
                       default=OPENAI_STRIKES_MODEL,
                       help=f'OpenAI model for strike analysis (default: {OPENAI_STRIKES_MODEL})')
    parser.add_argument('--datemodel', 
                       default=OPENAI_DATE_MODEL,
                       help=f'OpenAI model for date extraction (default: {OPENAI_DATE_MODEL})')
    
    return parser.parse_args()


def main():
    # Parse arguments
    args = parse_arguments()
    
    input_folder = args.input_folder
    output_folder = args.output_folder
    force_reprocess = args.force
    date_model = args.datemodel
    strikes_model = args.strikemodel
    
    # Validate input
    if not os.path.exists(input_folder):
        print(f"âŒ Input folder not found: {input_folder}")
        sys.exit(1)
    
    # Create output directory
    os.makedirs(output_folder, exist_ok=True)
    
    print("ğŸš€ Starting Strike LLM Cleaner...")
    print(f"ğŸ“ Input folder: {input_folder}")
    print(f"ğŸ“ Output folder: {output_folder}")
    print(f"ğŸ¤– OpenAI date model: {date_model}")
    print(f"ğŸ¤– OpenAI strikes model: {strikes_model}")
    print(f"ğŸ”„ Force reprocessing: {'Yes' if force_reprocess else 'No (will skip existing files)'}")
    
    # Setup OpenAI client
    try:
        client = setup_openai_client()
        print("âœ… OpenAI client initialized")
    except Exception as e:
        print(f"âŒ Failed to initialize OpenAI client: {e}")
        sys.exit(1)
    
    # Find all JSON files in input folder
    json_files = []
    for root, dirs, files in os.walk(input_folder):
        for file in files:
            if file.lower().endswith('.json'):
                json_files.append(os.path.join(root, file))
    
    if not json_files:
        print("âŒ No JSON files found in input folder!")
        sys.exit(1)
    
    # Sort files for consistent processing
    json_files.sort()
    
    # Process all files
    print(f"\nğŸ“‹ Processing {len(json_files)} JSON files...")
    processed_count = 0
    skipped_count = 0
    total_strikes = 0
    
    for i, input_path in enumerate(json_files):
        print(f"\n[{i+1}/{len(json_files)}] " + "="*60)
        
        # Generate output path
        input_filename = os.path.basename(input_path)
        output_filename = generate_output_filename(input_filename)
        output_path = os.path.join(output_folder, output_filename)
        
        # Check if output file already exists
        if os.path.exists(output_path) and not force_reprocess:
            print(f"    â­ï¸  Skipping: {input_filename} (output already exists)")
            skipped_count += 1
            
            # Still count strikes in existing output file for final summary
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    output_data = json.load(f)
                total_strikes += len(output_data.get("strikes", []))
            except:
                pass
            continue
        
        # Process the file
        if process_file(client, input_path, output_path, input_folder, date_model, strikes_model):
            processed_count += 1
            
            # Count strikes in output file
            try:
                with open(output_path, 'r', encoding='utf-8') as f:
                    output_data = json.load(f)
                total_strikes += len(output_data.get("strikes", []))
            except:
                pass
    
    # Final summary
    print(f"\n{'='*80}")
    print(f"ğŸ‰ Processing complete!")
    print(f"âœ… Successfully processed: {processed_count} files")
    print(f"â­ï¸  Skipped (already exist): {skipped_count} files")
    print(f"ğŸ“Š Total files: {len(json_files)}")
    print(f"ğŸ“Š Total strikes in all output: {total_strikes}")
    print(f"ğŸ“ Output files saved to: {output_folder}")
    
    if processed_count == 0 and skipped_count == 0:
        print("âš ï¸  No files were processed.")
        print("   Check the input files and OpenAI API configuration.")
    elif processed_count == 0 and skipped_count > 0:
        print("â„¹ï¸  All files were skipped (output already exists).")
        print("   Use --force argument to reprocess existing files.")


if __name__ == "__main__":
    main()